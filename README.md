# Activation-Functions
Activation functions are applied on the dot product of the input and weight.There are several activation functions and they are chosen based on the layer they are being used on (Hidden layer or Output layer) and the type of problem statement (Regression or classification)
this file contains the advantages and disadvantages of different activatation functions when they are applied on the dot product of the weight and input in forward propagation and when we calcualte the derivative of activation function with respect to weight/input in backward propogation in order to update the weights.
